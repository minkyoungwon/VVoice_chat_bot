
# TTS ì†ë„ ìµœì í™” ê°œì„ ì•ˆ - ìºì‹± ë° ë³‘ë ¬ ì²˜ë¦¬
import asyncio
import hashlib
import pickle
import os
import time
from typing import Dict, Optional, List
from concurrent.futures import ThreadPoolExecutor
import numpy as np
import torch
import torchaudio
from pathlib import Path

class AdvancedTTSCache:
    """ê³ ê¸‰ TTS ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, cache_dir: str = "cache/tts", max_cache_size_gb: float = 2.0):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_cache_size = max_cache_size_gb * 1024 * 1024 * 1024  # GB to bytes
        self.memory_cache: Dict[str, np.ndarray] = {}
        self.cache_metadata: Dict[str, Dict] = {}
        self.access_times: Dict[str, float] = {}
        
        # ìºì‹œ í†µê³„
        self.cache_hits = 0
        self.cache_misses = 0
        
        self._load_cache_index()
    
    def _get_cache_key(self, text: str, model: str, settings: Dict) -> str:
        """í…ìŠ¤íŠ¸ì™€ ì„¤ì •ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        # ì¤‘ìš”í•œ TTS ì„¤ì •ë§Œ í‚¤ì— í¬í•¨
        key_data = {
            'text': text.strip().lower(),
            'model': model,
            'language': settings.get('language', 'ko'),
            'emotion': settings.get('emotion', []),
            'speaking_rate': settings.get('speaking_rate', 15.0),
            'pitch_std': settings.get('pitch_std', 20.0)
        }
        key_str = str(sorted(key_data.items()))
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _load_cache_index(self):
        """ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œ"""
        index_file = self.cache_dir / "cache_index.pkl"
        if index_file.exists():
            try:
                with open(index_file, 'rb') as f:
                    data = pickle.load(f)
                    self.cache_metadata = data.get('metadata', {})
                    self.access_times = data.get('access_times', {})
                print(f"ğŸ“‚ ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œë¨: {len(self.cache_metadata)}ê°œ í•­ëª©")
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _save_cache_index(self):
        """ìºì‹œ ì¸ë±ìŠ¤ ì €ì¥"""
        index_file = self.cache_dir / "cache_index.pkl"
        try:
            with open(index_file, 'wb') as f:
                pickle.dump({
                    'metadata': self.cache_metadata,
                    'access_times': self.access_times
                }, f)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_cached_audio(self, text: str, model: str, settings: Dict) -> Optional[np.ndarray]:
        """ìºì‹œëœ ì˜¤ë””ì˜¤ ì¡°íšŒ"""
        cache_key = self._get_cache_key(text, model, settings)
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ë¨¼ì € í™•ì¸
        if cache_key in self.memory_cache:
            self.access_times[cache_key] = time.time()
            self.cache_hits += 1
            print(f"ğŸ¯ ë©”ëª¨ë¦¬ ìºì‹œ íˆíŠ¸: {text[:30]}...")
            return self.memory_cache[cache_key]
        
        # ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
        cache_file = self.cache_dir / f"{cache_key}.npz"
        if cache_file.exists():
            try:
                data = np.load(cache_file)
                audio_data = data['audio']
                
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥ (LRU ë°©ì‹)
                self._add_to_memory_cache(cache_key, audio_data)
                
                self.access_times[cache_key] = time.time()
                self.cache_hits += 1
                print(f"ğŸ’¾ ë””ìŠ¤í¬ ìºì‹œ íˆíŠ¸: {text[:30]}...")
                return audio_data
                
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                cache_file.unlink(missing_ok=True)
        
        self.cache_misses += 1
        return None
    
    def save_cached_audio(self, text: str, model: str, settings: Dict, audio_data: np.ndarray, sample_rate: int):
        """ì˜¤ë””ì˜¤ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_key = self._get_cache_key(text, model, settings)
        
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì¶”ê°€
        self._add_to_memory_cache(cache_key, audio_data)
        
        # ë””ìŠ¤í¬ ìºì‹œì— ì €ì¥
        cache_file = self.cache_dir / f"{cache_key}.npz"
        try:
            np.savez_compressed(
                cache_file,
                audio=audio_data,
                sample_rate=sample_rate,
                text=text,
                model=model
            )
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self.cache_metadata[cache_key] = {
                'text': text[:100],  # ì²« 100ìë§Œ ì €ì¥
                'model': model,
                'file_size': cache_file.stat().st_size,
                'created_at': time.time(),
                'sample_rate': sample_rate
            }
            self.access_times[cache_key] = time.time()
            
            print(f"ğŸ’¾ ìºì‹œ ì €ì¥ë¨: {text[:30]}... ({len(audio_data)} samples)")
            
            # ìºì‹œ í¬ê¸° ê´€ë¦¬
            self._manage_cache_size()
            
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _add_to_memory_cache(self, cache_key: str, audio_data: np.ndarray):
        """ë©”ëª¨ë¦¬ ìºì‹œì— ì¶”ê°€ (LRU ë°©ì‹)"""
        # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì œí•œ (50MB)
        max_memory_items = 100
        if len(self.memory_cache) >= max_memory_items:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times.get(k, 0))
            if oldest_key in self.memory_cache:
                del self.memory_cache[oldest_key]
        
        self.memory_cache[cache_key] = audio_data.copy()
    
    def _manage_cache_size(self):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬"""
        total_size = sum(
            self.cache_dir.joinpath(f"{key}.npz").stat().st_size 
            for key in self.cache_metadata 
            if self.cache_dir.joinpath(f"{key}.npz").exists()
        )
        
        if total_size > self.max_cache_size:
            # ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ë¶€í„° ì‚­ì œ
            sorted_keys = sorted(
                self.cache_metadata.keys(), 
                key=lambda k: self.access_times.get(k, 0)
            )
            
            for key in sorted_keys:
                cache_file = self.cache_dir / f"{key}.npz"
                if cache_file.exists():
                    file_size = cache_file.stat().st_size
                    cache_file.unlink()
                    total_size -= file_size
                    
                    # ë©”íƒ€ë°ì´í„°ì—ì„œë„ ì œê±°
                    del self.cache_metadata[key]
                    if key in self.access_times:
                        del self.access_times[key]
                    if key in self.memory_cache:
                        del self.memory_cache[key]
                    
                    if total_size <= self.max_cache_size * 0.8:  # 80%ê¹Œì§€ ì •ë¦¬
                        break
            
            self._save_cache_index()
            print(f"ğŸ§¹ ìºì‹œ ì •ë¦¬ ì™„ë£Œ. í˜„ì¬ í¬ê¸°: {total_size / 1024 / 1024:.1f}MB")
    
    def get_cache_stats(self) -> Dict:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': f"{hit_rate:.1f}%",
            'memory_cache_size': len(self.memory_cache),
            'disk_cache_size': len(self.cache_metadata),
            'total_cache_size_mb': sum(
                meta.get('file_size', 0) for meta in self.cache_metadata.values()
            ) / 1024 / 1024
        }


class ParallelTTSProcessor:
    """ë³‘ë ¬ TTS ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_workers: int = 2):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.text_splitter = SmartTextSplitter()
    
    async def process_long_text_parallel(self, text: str, tts_function, **kwargs) -> List[np.ndarray]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_chunks = self.text_splitter.split_text(text)
        
        if len(text_chunks) <= 1:
            # ë¶„í• í•  í•„ìš” ì—†ëŠ” ê²½ìš°
            result = await tts_function(text, **kwargs)
            return [result] if result is not None else []
        
        print(f"ğŸ”„ í…ìŠ¤íŠ¸ë¥¼ {len(text_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬")
        
        # ë³‘ë ¬ ì²˜ë¦¬
        loop = asyncio.get_event_loop()
        tasks = []
        
        for i, chunk in enumerate(text_chunks):
            task = loop.run_in_executor(
                self.executor,
                lambda c=chunk, i=i: asyncio.run(tts_function(c, chunk_id=i, **kwargs))
            )
            tasks.append(task)
        
        # ëª¨ë“  ì²­í¬ ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì •ë¦¬ (ì˜ˆì™¸ ì œê±°)
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âš ï¸ ì²­í¬ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
            elif result is not None:
                valid_results.append(result)
        
        return valid_results
    
    def combine_audio_chunks(self, audio_chunks: List[np.ndarray], sample_rate: int) -> np.ndarray:
        """ì˜¤ë””ì˜¤ ì²­í¬ë“¤ì„ í•˜ë‚˜ë¡œ ê²°í•©"""
        if not audio_chunks:
            return np.array([])
        
        if len(audio_chunks) == 1:
            return audio_chunks[0]
        
        # ì²­í¬ ì‚¬ì´ì— ì§§ì€ ë¬´ìŒ ì¶”ê°€ (ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°)
        silence_duration = 0.1  # 100ms
        silence_samples = int(sample_rate * silence_duration)
        silence = np.zeros(silence_samples, dtype=audio_chunks[0].dtype)
        
        combined = []
        for i, chunk in enumerate(audio_chunks):
            combined.append(chunk)
            if i < len(audio_chunks) - 1:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ë¬´ìŒ ì¶”ê°€
                combined.append(silence)
        
        return np.concatenate(combined)


class SmartTextSplitter:
    """ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""
    
    def __init__(self, max_chunk_length: int = 100, min_chunk_length: int = 20):
        self.max_chunk_length = max_chunk_length
        self.min_chunk_length = min_chunk_length
    
    def split_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
        if len(text) <= self.max_chunk_length:
            return [text]
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = self._split_into_sentences(text)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ ê¸¸ì´ í™•ì¸
            test_chunk = current_chunk + (" " if current_chunk else "") + sentence
            
            if len(test_chunk) <= self.max_chunk_length:
                current_chunk = test_chunk
            else:
                # í˜„ì¬ ì²­í¬ê°€ ë„ˆë¬´ ì§§ì§€ ì•Šìœ¼ë©´ ì €ì¥
                if len(current_chunk) >= self.min_chunk_length:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence
                else:
                    # í˜„ì¬ ì²­í¬ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸´ ë¬¸ì¥ê³¼ í•¨ê»˜ ì €ì¥
                    current_chunk = test_chunk
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        import re
        
        # í•œêµ­ì–´ì™€ ì˜ì–´ ë¬¸ì¥ êµ¬ë¶„ì
        sentence_endings = r'[.!?à¥¤à¥¤]+\s+'
        sentences = re.split(sentence_endings, text.strip())
        
        # ë¹ˆ ë¬¸ì¥ ì œê±°
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences


class ModelWarmupManager:
    """ëª¨ë¸ ì›œì—… ê´€ë¦¬ì"""
    
    def __init__(self):
        self.warmup_texts = [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤",
            "Hello world",
            "This is a test"
        ]
        self.warmed_up_models = set()
    
    async def warmup_model(self, model, model_name: str, make_cond_dict_func, device):
        """ëª¨ë¸ ì›œì—… (ì²« ì‹¤í–‰ ì‹œ ì§€ì—° ì‹œê°„ ë‹¨ì¶•)"""
        if model_name in self.warmed_up_models:
            return
        
        print(f"ğŸ”¥ ëª¨ë¸ ì›œì—… ì‹œì‘: {model_name}")
        start_time = time.time()
        
        try:
            # ì§§ì€ í…ìŠ¤íŠ¸ë“¤ë¡œ ëª‡ ë²ˆ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ì›œì—…
            for text in self.warmup_texts[:2]:  # ì²˜ìŒ 2ê°œë§Œ
                cond_dict = make_cond_dict_func(
                    text=text,
                    language="ko",
                    speaker=None,
                    emotion=[0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.3],
                    device=device
                )
                
                conditioning = model.prepare_conditioning(cond_dict)
                
                # ì‘ì€ í† í° ìˆ˜ë¡œ ë¹ ë¥´ê²Œ ìƒì„±
                with torch.no_grad():
                    codes = model.generate(
                        prefix_conditioning=conditioning,
                        max_new_tokens=50,  # ë§¤ìš° ì ì€ í† í°
                        cfg_scale=1.0,  # ë‚®ì€ CFGë¡œ ë¹ ë¥´ê²Œ
                        batch_size=1,
                        progress_bar=False,
                        disable_torch_compile=True
                    )
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del codes
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            warmup_time = time.time() - start_time
            self.warmed_up_models.add(model_name)
            print(f"âœ… ëª¨ë¸ ì›œì—… ì™„ë£Œ: {model_name} ({warmup_time:.2f}s)")
            
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì›œì—… ì‹¤íŒ¨: {e}")


# GPU ìµœì í™” ì„¤ì •
class GPUOptimizer:
    """GPU ìµœì í™” ê´€ë¦¬ì"""
    
    @staticmethod
    def optimize_gpu_settings():
        """GPU ìµœì í™” ì„¤ì • ì ìš©"""
        if not torch.cuda.is_available():
            return
        
        # CUDA ìµœì í™”
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        torch.cuda.empty_cache()
        
        # Mixed precision ê¶Œì¥ ì„¤ì •
        if torch.cuda.get_device_capability()[0] >= 7:  # V100, RTX ì‹œë¦¬ì¦ˆ ë“±
            print("ğŸš€ Mixed precision (Tensor Core) ìµœì í™” í™œì„±í™” ê¶Œì¥")
        
        print(f"ğŸ® GPU ìµœì í™” ì™„ë£Œ: {torch.cuda.get_device_name()}")
    
    @staticmethod
    def get_optimal_batch_size(model_name: str) -> int:
        """ëª¨ë¸ë³„ ìµœì  ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        if not torch.cuda.is_available():
            return 1
        
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        if "tiny" in model_name.lower():
            return min(4, int(gpu_memory_gb // 2))
        else:
            return min(2, int(gpu_memory_gb // 4))
