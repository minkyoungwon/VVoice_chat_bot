# voice_manager.py - ëª©ì†Œë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ

import os
import time
import base64
import tempfile
import logging
from typing import Dict, Optional, Any
import torch
import torchaudio
from zonos.model import Zonos

logger = logging.getLogger(__name__)

class VoiceManager:
    """ëª©ì†Œë¦¬ ì„ íƒ ë° ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, device: torch.device):
        self.device = device
        self.predefined_voices: Dict[str, str] = {}
        self.user_voice_cache: Dict[str, torch.Tensor] = {}
        self.speaker_embedding_cache: Dict[str, torch.Tensor] = {}
        self.load_predefined_voices()
    
    def load_predefined_voices(self):
        """ë¯¸ë¦¬ ì •ì˜ëœ ëª©ì†Œë¦¬ë“¤ ë¡œë“œ"""
        voice_dir = "assets/voices"
        
        # ê¸°ë³¸ ëª©ì†Œë¦¬ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(voice_dir, exist_ok=True)
        os.makedirs(f"{voice_dir}/user", exist_ok=True)
        
        if os.path.exists(voice_dir):
            for voice_file in os.listdir(voice_dir):
                if voice_file.endswith(('.wav', '.mp3', '.flac')):
                    voice_id = voice_file.split('.')[0]
                    self.predefined_voices[voice_id] = os.path.join(voice_dir, voice_file)
                    logger.info(f"ğŸ¤ ë¯¸ë¦¬ ì •ì˜ëœ ëª©ì†Œë¦¬ ë¡œë“œ: {voice_id}")
        
        # ìƒ˜í”Œ ëª©ì†Œë¦¬ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€
        if not self.predefined_voices:
            logger.warning("âš ï¸ assets/voices/ í´ë”ì— ëª©ì†Œë¦¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info("ğŸ’¡ .wav, .mp3, .flac íŒŒì¼ì„ assets/voices/ í´ë”ì— ì¶”ê°€í•˜ì„¸ìš”.")
    
    async def process_voice_request(self, voice_data: dict, model: Zonos) -> Optional[torch.Tensor]:
        """ëª©ì†Œë¦¬ ìš”ì²­ ì²˜ë¦¬"""
        try:
            # 1. ë¯¸ë¦¬ ì •ì˜ëœ ëª©ì†Œë¦¬ ID ì‚¬ìš©
            if "voice_id" in voice_data and voice_data["voice_id"]:
                voice_id = voice_data["voice_id"]
                if voice_id in self.predefined_voices:
                    logger.info(f"ğŸ¤ ë¯¸ë¦¬ ì •ì˜ëœ ëª©ì†Œë¦¬ ì‚¬ìš©: {voice_id}")
                    return await self._load_speaker_embedding(self.predefined_voices[voice_id], model, voice_id)
                else:
                    logger.warning(f"âš ï¸ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëª©ì†Œë¦¬ ID: {voice_id}")
            
            # 2. Base64 ì˜¤ë””ì˜¤ ì—…ë¡œë“œ ì²˜ë¦¬
            if "voice_audio_base64" in voice_data and voice_data["voice_audio_base64"]:
                logger.info("ğŸ¤ Base64 ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
                return await self._process_base64_audio(voice_data["voice_audio_base64"], model)
            
            # 3. ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì§ì ‘ ì§€ì •
            if "voice_file_path" in voice_data and voice_data["voice_file_path"]:
                file_path = voice_data["voice_file_path"]
                if os.path.exists(file_path):
                    logger.info(f"ğŸ¤ íŒŒì¼ ê²½ë¡œë¡œ ëª©ì†Œë¦¬ ë¡œë“œ: {file_path}")
                    return await self._load_speaker_embedding(file_path, model)
            
            # 4. ê¸°ë³¸ê°’: ëœë¤ ëª©ì†Œë¦¬ (None ë°˜í™˜)
            logger.info("ğŸ² ëœë¤ ëª©ì†Œë¦¬ ì‚¬ìš©")
            return None
            
        except Exception as e:
            logger.error(f"âŒ ëª©ì†Œë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_speaker_embedding(self, audio_path: str, model: Zonos, cache_key: str = None) -> torch.Tensor:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ìŠ¤í”¼ì»¤ ì„ë² ë”© ìƒì„± (ìºì‹± ì§€ì›)"""
        cache_key = cache_key or audio_path
        
        # ìºì‹œì—ì„œ í™•ì¸
        if cache_key in self.speaker_embedding_cache:
            logger.info(f"ğŸš€ ìºì‹œëœ ìŠ¤í”¼ì»¤ ì„ë² ë”© ì‚¬ìš©: {cache_key}")
            return self.speaker_embedding_cache[cache_key]
        
        try:
            logger.info(f"ğŸ“¥ ìŠ¤í”¼ì»¤ ì„ë² ë”© ìƒì„± ì¤‘: {audio_path}")
            wav, sr = torchaudio.load(audio_path)
            
            # ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜
            if wav.shape[0] > 1:
                wav = wav.mean(dim=0, keepdim=True)
            
            speaker_embedding = model.make_speaker_embedding(wav, sr)
            speaker_embedding = speaker_embedding.to(self.device, dtype=torch.bfloat16)
            
            # ìºì‹œì— ì €ì¥
            self.speaker_embedding_cache[cache_key] = speaker_embedding
            logger.info(f"âœ… ìŠ¤í”¼ì»¤ ì„ë² ë”© ìƒì„± ë° ìºì‹œ ì™„ë£Œ: {cache_key}")
            
            return speaker_embedding
            
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í”¼ì»¤ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise e
    
    async def _process_base64_audio(self, base64_data: str, model: Zonos) -> torch.Tensor:
        """Base64 ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬"""
        try:
            # Base64 í—¤ë” ì œê±°
            if base64_data.startswith('data:audio'):
                base64_data = base64_data.split(',')[1]
            
            # Base64 ë””ì½”ë”©
            audio_bytes = base64.b64decode(base64_data)
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ì²˜ë¦¬
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_bytes)
                temp_file_path = temp_file.name
            
            try:
                return await self._load_speaker_embedding(temp_file_path, model)
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if os.path.exists(temp_file_path):
                    os.unlink(temp_file_path)
                    
        except Exception as e:
            logger.error(f"âŒ Base64 ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise e
    
    def get_available_voices(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª©ì†Œë¦¬ ëª©ë¡ ë°˜í™˜"""
        return {
            "predefined_voices": list(self.predefined_voices.keys()),
            "cached_voices": list(self.speaker_embedding_cache.keys()),
            "upload_supported": True,
            "supported_formats": ["wav", "mp3", "flac"],
            "max_file_size_mb": 10,
            "voice_info": {
                voice_id: {
                    "file_path": file_path,
                    "cached": voice_id in self.speaker_embedding_cache
                }
                for voice_id, file_path in self.predefined_voices.items()
            }
        }
    
    async def add_voice_from_file(self, file_content: bytes, filename: str) -> str:
        """íŒŒì¼ë¡œë¶€í„° ìƒˆ ëª©ì†Œë¦¬ ì¶”ê°€"""
        try:
            # ê³ ìœ í•œ voice_id ìƒì„±
            voice_id = f"user_{int(time.time())}_{filename.split('.')[0]}"
            
            # íŒŒì¼ ì €ì¥
            file_path = f"assets/voices/user/{voice_id}.wav"
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            with open(file_path, "wb") as f:
                f.write(file_content)
            
            # ëª©ì†Œë¦¬ ë“±ë¡
            self.predefined_voices[voice_id] = file_path
            logger.info(f"âœ… ìƒˆ ëª©ì†Œë¦¬ ì¶”ê°€ë¨: {voice_id}")
            
            return voice_id
            
        except Exception as e:
            logger.error(f"âŒ ëª©ì†Œë¦¬ íŒŒì¼ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise e
    
    def clear_cache(self):
        """ìŠ¤í”¼ì»¤ ì„ë² ë”© ìºì‹œ í´ë¦¬ì–´"""
        self.speaker_embedding_cache.clear()
        logger.info("ğŸ§¹ ìŠ¤í”¼ì»¤ ì„ë² ë”© ìºì‹œ í´ë¦¬ì–´ë¨")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        return {
            "cached_embeddings": len(self.speaker_embedding_cache),
            "predefined_voices": len(self.predefined_voices),
            "cache_keys": list(self.speaker_embedding_cache.keys())
        }


# ê°ì • ì„¤ì • í—¬í¼ í´ë˜ìŠ¤
class EmotionManager:
    """ê°ì • ì„¤ì • ê´€ë¦¬"""
    
    EMOTION_PRESETS = {
        "happy": [0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1],
        "sad": [0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1],
        "angry": [0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.1, 0.1],
        "surprised": [0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.1, 0.1],
        "neutral": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3],
        "default": [0.3077, 0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.2564, 0.3077]
    }
    
    @staticmethod
    def get_emotion_vector(emotion_preset: str = None, emotion_custom: list = None) -> list:
        """ê°ì • ë²¡í„° ë°˜í™˜"""
        if emotion_custom:
            return emotion_custom
        elif emotion_preset and emotion_preset in EmotionManager.EMOTION_PRESETS:
            return EmotionManager.EMOTION_PRESETS[emotion_preset]
        else:
            return EmotionManager.EMOTION_PRESETS["default"]
    
    @staticmethod
    def get_available_presets() -> Dict[str, list]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ê°ì • í”„ë¦¬ì…‹ ë°˜í™˜"""
        return EmotionManager.EMOTION_PRESETS.copy()
