import os
import asyncio
import json
import logging
import time
from typing import Dict, Optional, Any, List
from contextlib import asynccontextmanager
# ê¸°ì¡´ importë“¤ ì•„ë˜ì— ì¶”ê°€
from tts_speed_optimization import AdvancedTTSCache, ParallelTTSProcessor, ModelWarmupManager, GPUOptimizer
import numpy as np

import torch
import torchaudio
import uvicorn

# ğŸ”§ PyTorch ì»´íŒŒì¼ ì™„ì „ ë¹„í™œì„±í™” (ë¹„í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
try:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
    # PyTorch ì»´íŒŒì¼ ì „ì—­ ë¹„í™œì„±í™”
    torch._dynamo.config.disable = True
    # Triton ì»´íŒŒì¼ëŸ¬ ì—ëŸ¬ ì–µì œ
    os.environ["PYTORCH_DISABLE_DYNAMO_COMPILATION"] = "1"
    os.environ["TORCH_COMPILE_DISABLE"] = "1"
except ImportError:
    pass

# PyTorch ì»´íŒŒì¼ ì™„ì „ ë¹„í™œì„±í™” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ["PYTORCH_DISABLE_DYNAMO_COMPILATION"] = "1"
os.environ["TORCH_COMPILE_DISABLE"] = "1"
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from huggingface_hub import login, HfApi

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ğŸ”¥ Hugging Face í† í° ì„¤ì •
hf_token = os.getenv("HUGGINGFACE_TOKEN")
if hf_token:
    try:
        login(token=hf_token, add_to_git_credential=False)
        print("âœ… Hugging Face í† í° ë¡œê·¸ì¸ ì„±ê³µ")
        
        # í† í° ìœ íš¨ì„± ê²€ì¦
        api = HfApi()
        whoami = api.whoami(token=hf_token)
        print(f"ğŸ”‘ Hugging Face ì‚¬ìš©ì: {whoami['name']}")
        
    except Exception as e:
        print(f"âŒ Hugging Face í† í° ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
        print("âš ï¸ Zonos ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    print("âš ï¸ HUGGINGFACE_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ .env íŒŒì¼ì— HUGGINGFACE_TOKENì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")

# ğŸ”¥ ë””ë°”ì´ìŠ¤ ì„¤ì • ë¡œì§ ê°œì„ 
def setup_device():
    """GPU/CPU ë””ë°”ì´ìŠ¤ ì„¤ì •"""
    device_setting = os.getenv("DEVICE", "auto").lower()
    force_cpu = os.getenv("FORCE_CPU", "false").lower() == "true"
    cuda_device_id = int(os.getenv("CUDA_DEVICE_ID", "0"))
    
    if force_cpu:
        device = torch.device("cpu")
        logger.info("ğŸ”§ ê°•ì œë¡œ CPU ì‚¬ìš©")
    elif device_setting == "cpu":
        device = torch.device("cpu")
        logger.info("ğŸ”§ CPU ë””ë°”ì´ìŠ¤ ì„ íƒë¨")
    elif device_setting == "cuda":
        if torch.cuda.is_available():
            device = torch.device(f"cuda:{cuda_device_id}")
            logger.info(f"ğŸš€ CUDA ë””ë°”ì´ìŠ¤ ì„ íƒë¨: {device} (GPU: {torch.cuda.get_device_name(cuda_device_id)})")
            
            # CUDA ìµœì í™” ì„¤ì •
            if os.getenv("TORCH_CUDNN_BENCHMARK", "true").lower() == "true":
                torch.backends.cudnn.benchmark = True
                logger.info("ğŸš€ cuDNN ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ í™œì„±í™”")
                
        else:
            device = torch.device("cpu")
            logger.warning("âš ï¸ CUDAê°€ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPUë¡œ fallback")
    else:  # auto
        if torch.cuda.is_available():
            device = torch.device(f"cuda:{cuda_device_id}")
            logger.info(f"ğŸš€ ìë™ ì„ íƒ: CUDA ë””ë°”ì´ìŠ¤ (GPU: {torch.cuda.get_device_name(cuda_device_id)})")
            torch.backends.cudnn.benchmark = True
        else:
            device = torch.device("cpu")
            logger.info("ğŸ”§ ìë™ ì„ íƒ: CPU ë””ë°”ì´ìŠ¤")
    
    return device

# Zonos ëª¨ë¸ import
from zonos.model import Zonos
from zonos.backbone import BACKBONES
from zonos.conditioning import make_cond_dict, supported_language_codes

# STT ì„œë¹„ìŠ¤ import
from stt_service import add_stt_routes, get_stt_service

# GPT ì„œë¹„ìŠ¤ import
from gpt_service import add_gpt_routes, initialize_gpt_service, get_gpt_service

# Firebase ì„œë¹„ìŠ¤ import
from firebase_service import (
    add_firebase_routes, 
    initialize_firebase_service, 
    log_user_message, 
    log_assistant_message, 
    log_system_message
)

# ëŒ€í™”í˜• WebSocket import
from conversation_websocket import set_dependencies, add_conversation_routes

# ëª©ì†Œë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ import
from voice_manager import VoiceManager, EmotionManager

# eSpeak í™˜ê²½ ì„¤ì •
espeak_path = os.getenv("ESPEAK_NG_PATH", r"C:\Program Files\eSpeak NG")
espeak_data_path = os.getenv("ESPEAK_NG_LIBRARY_PATH", r"C:\Program Files\eSpeak NG\espeak-ng-data")

os.environ["PATH"] += f";{espeak_path}"
os.environ["ESPEAK_NG_PATH"] = espeak_data_path

# ë¡œê¹… ì„¤ì •
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=getattr(logging, log_level))
logger = logging.getLogger(__name__)

# Hugging Face ë¡œê·¸ì¸ ìƒíƒœë¥¼ ë¡œê±°ë¡œ ë‹¤ì‹œ ì¶œë ¥
if hf_token:
    try:
        api = HfApi()
        whoami = api.whoami(token=hf_token)
        logger.info(f"âœ… Hugging Face ì¸ì¦ ì„±ê³µ: {whoami['name']}")
    except Exception as e:
        logger.error(f"âŒ Hugging Face ì¸ì¦ ì‹¤íŒ¨: {e}")
else:
    logger.warning("âš ï¸ HUGGINGFACE_TOKENì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = setup_device()

# ğŸ¯ ì§€ì›ë˜ëŠ” ëª¨ë¸ í™•ì¸ í•¨ìˆ˜
def get_supported_models() -> List[str]:
    """í˜„ì¬ ë°±ë³¸ì´ ì§€ì›í•˜ëŠ” ëª¨ë¸ë“¤ë§Œ ë°˜í™˜"""
    supported_models = []
    
    # ğŸ”¥ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” Transformer ëª¨ë¸ë§Œ ì¶”ê°€
    transformer_models = [
        "Zyphra/Zonos-v0.1-transformer"  # ìœ ì¼í•œ ì‘ë™í•˜ëŠ” ëª¨ë¸
        # "Zyphra/Zonos-v0.1-tiny"  # 404 ì—ëŸ¬ - ì¡´ì¬í•˜ì§€ ì•ŠìŒ
    ]
    
    for model in transformer_models:
        supported_models.append(model)
        
    logger.info(f"ğŸ¯ ì§€ì›ë˜ëŠ” ëª¨ë¸: {supported_models}")
    return supported_models

# ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
        self.enable_logging = os.getenv("ENABLE_PERFORMANCE_LOGGING", "true").lower() == "true"
    
    def start_timer(self, operation: str) -> str:
        """íƒ€ì´ë¨¸ ì‹œì‘"""
        timer_id = f"{operation}_{int(time.time() * 1000)}"
        self.metrics[timer_id] = {"start": time.time(), "operation": operation}
        return timer_id
    
    def end_timer(self, timer_id: str) -> float:
        """íƒ€ì´ë¨¸ ì¢…ë£Œ ë° ì‹œê°„ ë°˜í™˜"""
        if timer_id in self.metrics:
            elapsed = time.time() - self.metrics[timer_id]["start"]
            operation = self.metrics[timer_id]["operation"]
            
            if self.enable_logging:
                logger.info(f"â±ï¸ {operation}: {elapsed:.3f}ì´ˆ")
                
            return elapsed
        return 0.0
    
    def log_memory_usage(self, context: str = ""):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        if self.enable_logging and torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            logger.info(f"ğŸ§  GPU ë©”ëª¨ë¦¬ {context}: í• ë‹¹={allocated:.2f}GB, ì˜ˆì•½={reserved:.2f}GB")

perf_monitor = PerformanceMonitor()

# ê°œì„ ëœ ëª¨ë¸ ìºì‹œ í´ë˜ìŠ¤
class EnhancedModelCache:
    def __init__(self):
        self.models: Dict[str, Zonos] = {}
        self.current_model_type: Optional[str] = None
        self.speaker_embedding: Optional[torch.Tensor] = None
        self.speaker_audio_path: Optional[str] = None
        self.loading_progress: Dict[str, float] = {}
        self.loading_status: Dict[str, str] = {}
        self.supported_models = get_supported_models()
        self.warmup_completed = set()
        self.compilation_cache = {}
        
    def _validate_model(self, model_choice: str) -> str:
        """ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬ ë° ëŒ€ì²´ ëª¨ë¸ ì œì•ˆ"""
        if model_choice in self.supported_models:
            return model_choice
            
        # ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¸ ê²½ìš° ëŒ€ì²´ ëª¨ë¸ ì œì•ˆ
        if "hybrid" in model_choice.lower():
            replacement = "Zyphra/Zonos-v0.1-transformer"
            logger.warning(f"âš ï¸ {model_choice} ëª¨ë¸ì€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. {replacement}ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return replacement
        elif "tiny" in model_choice.lower():
            return "Zyphra/Zonos-v0.1-tiny"
        else:
            # ê¸°ë³¸ ëª¨ë¸ë¡œ fallback
            default_model = self.supported_models[0] if self.supported_models else "Zyphra/Zonos-v0.1-transformer"
            logger.warning(f"âš ï¸ {model_choice} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {default_model}ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return default_model
        
    async def load_model_with_progress(self, model_choice: str, websocket: WebSocket = None) -> Zonos:
        """í”„ë¡œê·¸ë ˆìŠ¤ë°”ì™€ í•¨ê»˜ ëª¨ë¸ ë¡œë“œ"""
        # ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬
        validated_model = self._validate_model(model_choice)
        
        if validated_model in self.models:
            self.current_model_type = validated_model
            if validated_model not in self.warmup_completed:
                await self._warmup_model(validated_model, websocket)
            
            if websocket:
                await websocket.send_json({
                    "type": "model_already_loaded",
                    "model": validated_model,
                    "memory_usage": self._get_memory_usage()
                })
            return self.models[validated_model]
        
        # íƒ€ì´ë¨¸ ì‹œì‘
        timer_id = perf_monitor.start_timer(f"model_load_{validated_model}")
        
        # ë¡œë”© ìƒíƒœ ì´ˆê¸°í™”
        self.loading_progress[validated_model] = 0.0
        self.loading_status[validated_model] = "ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘..."
        
        try:
            if websocket:
                await websocket.send_json({
                    "type": "model_loading_progress",
                    "model": validated_model,
                    "progress": 0.0,
                    "status": "ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘"
                })
            
            logger.info(f"ğŸ“¥ Loading {validated_model} model...")
            
            # ë‹¨ê³„ë³„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            steps = [
                (10, "ëª¨ë¸ êµ¬ì„± íŒŒì¼ í™•ì¸ ì¤‘..."),
                (25, "ê°€ì¤‘ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘..."),
                (50, "ëª¨ë¸ ì´ˆê¸°í™” ì¤‘..."),
                (75, f"{'GPU' if device.type == 'cuda' else 'CPU'} ë©”ëª¨ë¦¬ í• ë‹¹ ì¤‘..."),
                (90, "ëª¨ë¸ ì»´íŒŒì¼ ì¤‘..."),
                (100, "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            ]
            
            for i, (progress, status) in enumerate(steps):
                self.loading_progress[validated_model] = progress
                self.loading_status[validated_model] = status
                
                if websocket:
                    await websocket.send_json({
                        "type": "model_loading_progress",
                        "model": validated_model,
                        "progress": progress,
                        "status": status
                    })
                
                # ì‹¤ì œ ëª¨ë¸ ë¡œë”©ì€ 50% ì§€ì ì—ì„œ ìˆ˜í–‰
                if progress == 50:
                    try:
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (ë¡œë”© ì „)
                        perf_monitor.log_memory_usage("ëª¨ë¸ ë¡œë”© ì „")
                        
                        # ì‹¤ì œ ëª¨ë¸ ë¡œë“œ
                        model = Zonos.from_pretrained(validated_model, device=device)
                        model.requires_grad_(False).eval()
                        
                        # Mixed precision ì„¤ì •
                        if os.getenv("MIXED_PRECISION", "true").lower() == "true" and device.type == "cuda":
                            model = model.to(dtype=torch.bfloat16)
                            logger.info("ğŸš€ Mixed precision (bfloat16) ì ìš©ë¨")
                        
                        # PyTorch ì»´íŒŒì¼ ë¹„í™œì„±í™” (ëŒ€ì‹  Eager ëª¨ë“œ ì‚¬ìš©)
                        # ì»´íŒŒì¼ëŸ¬ ë¬¸ì œë¡œ ì¸í•´ ì™„ì „ ë¹„í™œì„±í™”
                        logger.info("ğŸš€ PyTorch Eager ëª¨ë“œ ì‚¬ìš© (ì»´íŒŒì¼ ë¹„í™œì„±í™”)")
                        
                        logger.info(f"âœ… Model {validated_model} loaded to device: {device}")
                        
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (ë¡œë”© í›„)
                        perf_monitor.log_memory_usage("ëª¨ë¸ ë¡œë”© í›„")
                        
                    except Exception as e:
                        logger.error(f"âŒ Failed to load model {validated_model}: {e}")
                        raise e
                
                # ê° ë‹¨ê³„ë§ˆë‹¤ ì•½ê°„ì˜ ì§€ì—° (ì‚¬ìš©ì ê²½í—˜ ê°œì„ )
                await asyncio.sleep(0.3 if i < len(steps) - 1 else 0.1)
            
            self.models[validated_model] = model
            self.current_model_type = validated_model
            
            # íƒ€ì´ë¨¸ ì¢…ë£Œ
            load_time = perf_monitor.end_timer(timer_id)
            
            logger.info(f"âœ… {validated_model} model loaded successfully in {load_time:.2f}s!")
            
            if websocket:
                await websocket.send_json({
                    "type": "model_loading_complete",
                    "model": validated_model,
                    "load_time": load_time,
                    "memory_usage": self._get_memory_usage(),
                    "device": str(device)
                })
            
            return model
            
        except Exception as e:
            self.loading_status[validated_model] = f"ë¡œë”© ì‹¤íŒ¨: {str(e)}"
            logger.error(f"âŒ Failed to load {validated_model}: {e}")
            
            if websocket:
                await websocket.send_json({
                    "type": "model_loading_error",
                    "model": validated_model,
                    "error": str(e)
                })
            
            raise e
    
    def _get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
        memory_info = {}
        if torch.cuda.is_available():
            memory_info["gpu_allocated"] = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_info["gpu_reserved"] = torch.cuda.memory_reserved() / 1024**3   # GB
            memory_info["gpu_total"] = torch.cuda.get_device_properties(0).total_memory / 1024**3
        return memory_info
    
    def load_model_if_needed(self, model_choice: str) -> Zonos:
        """ë™ê¸°ì‹ ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        validated_model = self._validate_model(model_choice)
        
        if validated_model not in self.models:
            timer_id = perf_monitor.start_timer(f"sync_model_load_{validated_model}")
            logger.info(f"ğŸ“¥ Loading {validated_model} model synchronously...")
            
            try:
                perf_monitor.log_memory_usage("ë™ê¸° ëª¨ë¸ ë¡œë”© ì „")
                
                model = Zonos.from_pretrained(validated_model, device=device)
                model.requires_grad_(False).eval()
                
                # Mixed precision ì„¤ì •
                if os.getenv("MIXED_PRECISION", "true").lower() == "true" and device.type == "cuda":
                    model = model.to(dtype=torch.bfloat16)
                
                self.models[validated_model] = model
                
                load_time = perf_monitor.end_timer(timer_id)
                perf_monitor.log_memory_usage("ë™ê¸° ëª¨ë¸ ë¡œë”© í›„")
                
                logger.info(f"âœ… {validated_model} model loaded successfully in {load_time:.2f}s!")
                
            except Exception as e:
                logger.error(f"âŒ Failed to load {validated_model}: {e}")
                raise e
        
        self.current_model_type = validated_model
        return self.models[validated_model]
    
    def get_speaker_embedding(self, audio_path: str) -> torch.Tensor:
        """ìŠ¤í”¼ì»¤ ì„ë² ë”© ìºì‹œ ê´€ë¦¬"""
        if audio_path != self.speaker_audio_path:
            current_model = self.models.get(self.current_model_type)
            if current_model is None:
                raise ValueError("No model loaded")
                
            wav, sr = torchaudio.load(audio_path)
            self.speaker_embedding = current_model.make_speaker_embedding(wav, sr)
            self.speaker_embedding = self.speaker_embedding.to(device, dtype=torch.bfloat16)
            self.speaker_audio_path = audio_path
            logger.info("ğŸ¤ Recomputed speaker embedding")
            
        return self.speaker_embedding
    
    async def _warmup_model(self, model_name: str, websocket: "WebSocket" = None):
        """ëª¨ë¸ ì›œì—… (ë¹„ë™ê¸°)"""
        if model_name in self.warmup_completed:
            return
        
        if websocket:
            await websocket.send_json({
                "type": "model_warmup_start", 
                "model": model_name,
                "status": "ëª¨ë¸ ì›œì—… ì¤‘..."
            })
        
        try:
            model = self.models[model_name]
            await warmup_manager.warmup_model(model, model_name, make_cond_dict, device)
            self.warmup_completed.add(model_name)
            
            if websocket:
                await websocket.send_json({
                    "type": "model_warmup_complete",
                    "model": model_name,
                    "status": "ì›œì—… ì™„ë£Œ"
                })
        except Exception as e:
            logger.warning(f"âš ï¸ ëª¨ë¸ ì›œì—… ì‹¤íŒ¨: {e}")
    
    



# ê¸€ë¡œë²Œ ëª¨ë¸ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
model_cache = EnhancedModelCache()

# ê¸€ë¡œë²Œ ëª©ì†Œë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
voice_manager = VoiceManager(device)



# ê¸°ì¡´ model_cache ì„ ì–¸ ì•„ë˜ì— ì¶”ê°€
tts_cache = AdvancedTTSCache(
    cache_dir=os.getenv("TTS_CACHE_DIR", "cache/tts"),
    max_cache_size_gb=float(os.getenv("TTS_CACHE_SIZE_GB", "2.0"))
)
parallel_processor = ParallelTTSProcessor(
    max_workers=int(os.getenv("TTS_MAX_WORKERS", "2"))
)
warmup_manager = ModelWarmupManager()

# GPU ìµœì í™” ì ìš©
GPUOptimizer.optimize_gpu_settings()






# WebSocket ì—°ê²° ê´€ë¦¬ì ê°œì„ 
class EnhancedConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.connection_info: Dict[str, Dict] = {}
    
    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections[client_id] = websocket
        self.connection_info[client_id] = {
            "connected_at": time.time(),
            "last_activity": time.time(),
            "status": "connected"
        }
        logger.info(f"ğŸ”— Client {client_id} connected to TTS WebSocket")
    
    def disconnect(self, client_id: str):
        if client_id in self.active_connections:
            del self.active_connections[client_id]
            del self.connection_info[client_id]
            logger.info(f"ğŸ”Œ Client {client_id} disconnected from TTS WebSocket")
    
    async def send_to_client(self, client_id: str, message: dict):
        """íŠ¹ì • í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë©”ì‹œì§€ ì „ì†¡"""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json(message)
                self.connection_info[client_id]["last_activity"] = time.time()
            except Exception as e:
                logger.error(f"âŒ Failed to send message to {client_id}: {e}")
                self.disconnect(client_id)

tts_manager = EnhancedConnectionManager()

# ğŸš€ ìš¸íŠ¸ë¼ ìµœì í™”ëœ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
async def ultra_optimized_stream_audio_generation(
    websocket: WebSocket, 
    model: Zonos, 
    conditioning: torch.Tensor,
    request_data: Dict[str, Any],
    format_type: str = "pcm",
    client_id: str = None
):
    """ìš¸íŠ¸ë¼ ìµœì í™”ëœ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°"""
    
    text = request_data.get("text", "")
    model_name = request_data.get("model", "")
    
    # 1. ìºì‹œ í™•ì¸ ë‹¨ê³„
    cache_settings = {
        'model': model_name,
        'language': request_data.get('language', 'ko'),
        'emotion': request_data.get('emotion', []),
        'speaking_rate': request_data.get('speaking_rate', 15.0),
        'pitch_std': request_data.get('pitch_std', 20.0),
        'cfg_scale': request_data.get('cfg_scale', 2.0)
    }
    
    cached_audio = tts_cache.get_cached_audio(text, model_name, cache_settings)
    
    if cached_audio is not None:
        # ìºì‹œ íˆíŠ¸! ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë°
        try:
            await websocket.send_json({
                "type": "cache_hit",
                "message": "ìºì‹œëœ ì˜¤ë””ì˜¤ ì‚¬ìš© (ì´ˆê³ ì†)",
                "latency": "~0.05s"
            })
            
            sr = model.autoencoder.sampling_rate
            await websocket.send_json({
                "type": "generation_metadata",
                "sample_rate": int(sr),
                "total_duration": len(cached_audio) / sr,
                "generation_time": 0.05,  # ìºì‹œ íˆíŠ¸ ì‹œê°„
                "latency": 0.05,
                "source": "cache",
                "performance": "ğŸš€ ìºì‹œ"
            })
            
            # ìºì‹œëœ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
            await _stream_cached_audio(websocket, cached_audio, sr, format_type)
            return
            
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            # ìºì‹œ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ìƒì„±ìœ¼ë¡œ fallback
    
    
    # 2. ê¸´ í…ìŠ¤íŠ¸ ë³‘ë ¬ ì²˜ë¦¬ í™•ì¸
    
    if len(text) > int(os.getenv("PARALLEL_TEXT_THRESHOLD", "150")):
        try:
            await websocket.send_json({
                "type": "parallel_processing",
                "message": "ê¸´ í…ìŠ¤íŠ¸ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘...",
                "text_length": len(text)
            })
            
            audio_chunks = await _process_text_parallel(model, conditioning, request_data)
            
            if audio_chunks:
                # ì˜¤ë””ì˜¤ ì²­í¬ ê²°í•©
                sr = model.autoencoder.sampling_rate
                combined_audio = parallel_processor.combine_audio_chunks(audio_chunks, sr)
                
                # ìºì‹œì— ì €ì¥
                tts_cache.save_cached_audio(text, model_name, cache_settings, combined_audio, sr)
                
                # ê²°í•©ëœ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
                await _stream_generated_audio(websocket, combined_audio, sr, format_type, "parallel")
                return
                
        except Exception as e:
            logger.warning(f"âš ï¸ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨, ì¼ë°˜ ì²˜ë¦¬ë¡œ fallback: {e}")
    
    
    # 3. ì¼ë°˜ ìƒì„± (ìµœì í™” ì ìš©)
    
    timer_id = perf_monitor.start_timer("optimized_audio_generation")
    
    try:
        await websocket.send_json({
            "type": "generation_started",
            "text": text,
            "model": model_name,
            "mode": "optimized_single"
        })
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        perf_monitor.log_memory_usage("ìµœì í™” ìƒì„± ì „")
        
        # ğŸš€ ìµœì í™”ëœ ìƒì„± íŒŒë¼ë¯¸í„°
        optimal_batch_size = GPUOptimizer.get_optimal_batch_size(model_name)
        max_new_tokens = min(86 * 30, len(text) * 12)  # ë” íš¨ìœ¨ì ì¸ í† í° ê³„ì‚°
        
        # Mixed precision ì‚¬ìš©
        with torch.autocast(device_type=device.type, enabled=device.type == 'cuda'):
            codes = model.generate(
                prefix_conditioning=conditioning,
                audio_prefix_codes=None,
                max_new_tokens=max_new_tokens,
                cfg_scale=request_data.get("cfg_scale", 2.0),
                batch_size=optimal_batch_size,
                sampling_params=dict(
                    min_p=0.1,
                    temperature=0.8,  # ì•½ê°„ ë‚®ì¶°ì„œ ì•ˆì •ì„± í–¥ìƒ
                ),
                progress_bar=False,
                disable_torch_compile=True,
            )
        
        # ì˜¤ë””ì˜¤ ë””ì½”ë”©
        wav_out = model.autoencoder.decode(codes).cpu().detach()
        if wav_out.dim() == 2 and wav_out.size(0) > 1:
            wav_out = wav_out[0:1, :]
        
        audio_data = wav_out.squeeze().numpy()
        sr = model.autoencoder.sampling_rate
        
        generation_time = perf_monitor.end_timer(timer_id)
        perf_monitor.log_memory_usage("ìµœì í™” ìƒì„± í›„")
        
        # ìºì‹œì— ì €ì¥
        tts_cache.save_cached_audio(text, model_name, cache_settings, audio_data, sr)
        
        # ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
        await _stream_generated_audio(websocket, audio_data, sr, format_type, "optimized", generation_time)
        
    except Exception as e:
        logger.error(f"âŒ ìµœì í™” ì˜¤ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
        await websocket.send_json({
            "type": "generation_error",
            "error": f"ìµœì í™” ìƒì„± ì‹¤íŒ¨: {str(e)}",
            "error_code": "OPTIMIZED_GENERATION_ERROR"
        })

# ë³´ì¡° í•¨ìˆ˜ë“¤
async def _stream_cached_audio(websocket: WebSocket, audio_data: np.ndarray, sr: int, format_type: str):
    """ìºì‹œëœ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°"""
    chunk_duration = 0.05  # ìºì‹œëŠ” ë” ì‘ì€ ì²­í¬ë¡œ ë¹ ë¥´ê²Œ
    chunk_size = int(sr * chunk_duration)
    
    for i in range(0, len(audio_data), chunk_size):
        chunk = audio_data[i:i + chunk_size]
        
        if format_type == "pcm":
            chunk_int16 = (chunk * 32767).astype('int16')
            await websocket.send_bytes(chunk_int16.tobytes())
        else:
            await websocket.send_bytes(chunk.astype('float32').tobytes())
        
        await asyncio.sleep(0.005)  # ë” ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë°

async def _stream_generated_audio(websocket: WebSocket, audio_data: np.ndarray, sr: int, format_type: str, source: str, generation_time: float = 0):
    """ìƒì„±ëœ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°"""
    audio_duration = len(audio_data) / sr
    rtf = generation_time / audio_duration if audio_duration > 0 else 0
    
    await websocket.send_json({
        "type": "generation_metadata",
        "sample_rate": int(sr),
        "total_duration": audio_duration,
        "generation_time": generation_time,
        "latency": generation_time,
        "rtf": rtf,
        "source": source,
        "performance": "ğŸš€ ìµœì í™”ë¨" if rtf < 0.5 else "âš¡ ë¹ ë¦„" if rtf < 1.0 else "âš ï¸ ëŠë¦¼"
    })
    
    chunk_duration = float(os.getenv("TTS_CHUNK_DURATION", "0.1"))
    chunk_size = int(sr * chunk_duration)
    
    for i in range(0, len(audio_data), chunk_size):
        chunk = audio_data[i:i + chunk_size]
        
        if format_type == "pcm":
            chunk_int16 = (chunk * 32767).astype('int16')
            await websocket.send_bytes(chunk_int16.tobytes())
        else:
            await websocket.send_bytes(chunk.astype('float32').tobytes())
        
        await asyncio.sleep(0.01)

async def _process_text_parallel(model: Zonos, base_conditioning: torch.Tensor, request_data: Dict) -> List[np.ndarray]:
    """í…ìŠ¤íŠ¸ ë³‘ë ¬ ì²˜ë¦¬"""
    text = request_data.get("text", "")
    text_chunks = parallel_processor.text_splitter.split_text(text)
    
    if len(text_chunks) <= 1:
        return []
    
    async def generate_chunk(chunk_text: str, chunk_id: int = 0):
        # ì²­í¬ë³„ ì»¨ë””ì…”ë‹ ìƒì„±
        cond_dict = make_cond_dict(
            text=chunk_text,
            language=request_data.get("language", "ko"),
            speaker=None,
            emotion=request_data.get("emotion", [0.3077, 0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.2564, 0.3077]),
            fmax=request_data.get("fmax", 22050.0),
            pitch_std=request_data.get("pitch_std", 20.0),
            speaking_rate=request_data.get("speaking_rate", 15.0),
            vqscore_8=request_data.get("vqscore_8", [0.78] * 8),
            dnsmos_ovrl=request_data.get("dnsmos_ovrl", 4.0),
            device=device,
            unconditional_keys={"vqscore_8", "dnsmos_ovrl"}
        )
        
        conditioning = model.prepare_conditioning(cond_dict)
        
        # ì²­í¬ ìƒì„±
        max_new_tokens = min(86 * 20, len(chunk_text) * 10)
        
        with torch.autocast(device_type=device.type, enabled=device.type == 'cuda'):
            codes = model.generate(
                prefix_conditioning=conditioning,
                max_new_tokens=max_new_tokens,
                cfg_scale=request_data.get("cfg_scale", 2.0),
                batch_size=1,
                sampling_params=dict(min_p=0.1),
                progress_bar=False,
                disable_torch_compile=True,
            )
        
        wav_out = model.autoencoder.decode(codes).cpu().detach()
        if wav_out.dim() == 2 and wav_out.size(0) > 1:
            wav_out = wav_out[0:1, :]
        
        return wav_out.squeeze().numpy()
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
    return await parallel_processor.process_long_text_parallel(text, generate_chunk)

# ê¸°ì¡´ í•¨ìˆ˜ (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
async def enhanced_stream_audio_generation(
    websocket: WebSocket, 
    model: Zonos, 
    conditioning: torch.Tensor,
    request_data: Dict[str, Any],
    format_type: str = "pcm",
    client_id: str = None
):
    """ê¸°ì¡´ í•¨ìˆ˜ -> ìµœì í™” í•¨ìˆ˜ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    return await ultra_optimized_stream_audio_generation(
        websocket, model, conditioning, request_data, format_type, client_id
    )

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI ì•± ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    logger.info("ğŸš€ Starting up Enhanced Zonos FastAPI server...")
    
    # ë””ë°”ì´ìŠ¤ ì •ë³´ ì¶œë ¥
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        logger.info(f"ğŸ® GPU ì •ë³´: {torch.cuda.get_device_name()}")
        logger.info(f"ğŸ§  GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡
    supported_models = get_supported_models()
    app.state.supported_models = supported_models
    logger.info(f"ğŸ¯ ì§€ì› ëª¨ë¸: {supported_models}")
    
    # ê¸°ë³¸ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ (ì„ íƒì‚¬í•­)
    try_preload = os.getenv("PRELOAD_DEFAULT_MODEL", "false").lower() == "true"
    if supported_models and try_preload:
        try:
            logger.info("ğŸ“¥ ê¸°ë³¸ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ ì¤‘...")
            default_model = os.getenv("DEFAULT_TTS_MODEL", supported_models[0])
            await model_cache.load_model_with_progress(default_model)
            logger.info("âœ… ê¸°ë³¸ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ê¸°ë³¸ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # GPT ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    gpt_api_key = os.getenv("DEEPSEEK_API_KEY")
    if gpt_api_key:
        initialize_gpt_service(gpt_api_key)
        logger.info("âœ… GPT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ DEEPSEEK_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # Firebase ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    firebase_creds_path = os.getenv("FIREBASE_CREDENTIALS_PATH")
    if firebase_creds_path and os.path.exists(firebase_creds_path):
        initialize_firebase_service(firebase_creds_path)
        logger.info("âœ… Firebase ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ Firebase credentials not found, logging disabled")
    
    # ëŒ€í™”í˜• WebSocket ì˜ì¡´ì„± ì£¼ì…
    set_dependencies({
        'model_cache': model_cache,
        'make_cond_dict': make_cond_dict,
        'device': device,
        'log_user_message': log_user_message,
        'log_assistant_message': log_assistant_message,
        'log_system_message': log_system_message,
        'get_gpt_service': get_gpt_service,
        'get_stt_service': get_stt_service
    })
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    logger.info("ğŸ›‘ Shutting down Enhanced Zonos FastAPI server...")
    model_cache.models.clear()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Enhanced Zonos TTS API",
    description="ì‹¤ì‹œê°„ WebSocket ê¸°ë°˜ Text-to-Speech API with GPU/CPU Support",
    version="2.1.0",
    lifespan=lifespan
)

# ì„œë¹„ìŠ¤ ë¼ìš°í„°ë“¤ ì¶”ê°€
add_stt_routes(app)
add_gpt_routes(app)
add_firebase_routes(app)
add_conversation_routes(app)

# CORS ì„¤ì •
allowed_origins = os.getenv("ALLOWED_ORIGINS", '["*"]')
try:
    import ast
    origins_list = ast.literal_eval(allowed_origins)
except:
    origins_list = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins_list,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response ëª¨ë¸ë“¤
class TTSRequest(BaseModel):
    text: str
    model: str = "Zyphra/Zonos-v0.1-transformer"
    language: str = "ko"
    format: str = "pcm"
    emotion: list[float] = [0.3077, 0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.2564, 0.3077]
    fmax: float = 22050.0
    pitch_std: float = 20.0
    speaking_rate: float = 15.0
    vqscore_8: list[float] = [0.78] * 8
    dnsmos_ovrl: float = 4.0
    cfg_scale: float = 2.0
    seed: int = 420
    randomize_seed: bool = True

class ModelInfoResponse(BaseModel):
    supported_models: list[str]
    current_model: Optional[str]
    device: str
    device_info: Dict[str, Any]

# REST API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/", response_model=dict)
async def root():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Enhanced Zonos TTS API v2.1.0",
        "status": "running",
        "device": str(device),
        "supported_models": getattr(app.state, 'supported_models', []),
        "memory_usage": model_cache._get_memory_usage(),
        "features": {
            "gpu_support": torch.cuda.is_available(),
            "mixed_precision": os.getenv("MIXED_PRECISION", "true").lower() == "true",
            "torch_compile": os.getenv("ENABLE_TORCH_COMPILE", "true").lower() == "true"
        }
    }

@app.get("/api/models", response_model=ModelInfoResponse)
async def get_models():
    """ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    device_info = {
        "type": device.type,
        "name": torch.cuda.get_device_name() if torch.cuda.is_available() else "CPU"
    }
    
    if torch.cuda.is_available():
        props = torch.cuda.get_device_properties(0)
        device_info.update({
            "total_memory_gb": props.total_memory / 1024**3,
            "compute_capability": f"{props.major}.{props.minor}"
        })
    
    return ModelInfoResponse(
        supported_models=getattr(app.state, 'supported_models', []),
        current_model=model_cache.current_model_type,
        device=str(device),
        device_info=device_info
    )

@app.get("/api/tts/status")
async def get_tts_status():
    """TTS ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
    return {
        "status": "running",
        "loaded_models": list(model_cache.models.keys()),
        "current_model": model_cache.current_model_type,
        "memory_usage": model_cache._get_memory_usage(),
        "active_connections": len(tts_manager.active_connections),
        "device": str(device),
        "supported_models": getattr(app.state, 'supported_models', []),
        "performance_features": {
            "gpu_enabled": device.type == "cuda",
            "mixed_precision": os.getenv("MIXED_PRECISION", "true").lower() == "true",
            "torch_compile": os.getenv("ENABLE_TORCH_COMPILE", "true").lower() == "true",
            "cudnn_benchmark": torch.backends.cudnn.benchmark if torch.cuda.is_available() else False
        }
    }

@app.post("/api/tts/preload")
async def preload_model(model_name: str):
    """ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ"""
    try:
        await model_cache.load_model_with_progress(model_name)
        return {
            "status": "success",
            "message": f"Model {model_name} preloaded successfully",
            "memory_usage": model_cache._get_memory_usage()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ğŸ¤ ëª©ì†Œë¦¬ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/api/tts/voices")
async def get_available_voices():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª©ì†Œë¦¬ ëª©ë¡ ë°˜í™˜"""
    try:
        logger.info("ğŸ¤ ëª©ì†Œë¦¬ ëª©ë¡ ìš”ì²­ ìˆ˜ì‹ ")
        
        # voice_manager ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
        if voice_manager is None:
            logger.error("âŒ VoiceManager ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return JSONResponse(
                status_code=500,
                content={
                    "status": "error", 
                    "message": "VoiceManager ì´ˆê¸°í™” ì‹¤íŒ¨",
                    "data": {"predefined_voices": [], "voice_info": {}}
                }
            )
        
        voices_info = voice_manager.get_available_voices()
        logger.info(f"âœ… ëª©ì†Œë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {len(voices_info.get('predefined_voices', []))}ê°œ")
        
        response_data = {
            "status": "success",
            "data": voices_info,
            "message": f"í™•ì¸ {len(voices_info.get('predefined_voices', []))}ê°œ ëª©ì†Œë¦¬ ì‚¬ìš© ê°€ëŠ¥",
            "timestamp": time.time()
        }
        
        logger.info(f"ğŸ“¤ ëª©ì†Œë¦¬ API ì‘ë‹µ ì „ì†¡: {response_data}")
        return JSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"âŒ ëª©ì†Œë¦¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": f"ëª©ì†Œë¦¬ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {str(e)}",
                "data": {"predefined_voices": [], "voice_info": {}},
                "error_type": type(e).__name__
            }
        )

@app.post("/api/tts/upload-voice")
async def upload_voice_file(file: UploadFile = File(...)):
    """ëª©ì†Œë¦¬ íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        logger.info(f"ğŸ“¤ ëª©ì†Œë¦¬ ì—…ë¡œë“œ ìš”ì²­: {file.filename}, {file.content_type}")
        
        # voice_manager ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
        if voice_manager is None:
            logger.error("âŒ VoiceManager ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return JSONResponse(
                status_code=500,
                content={
                    "status": "error",
                    "message": "VoiceManager ì´ˆê¸°í™” ì‹¤íŒ¨"
                }
            )
        
        # íŒŒì¼ ê²€ì¦
        if not file.content_type or not file.content_type.startswith('audio/'):
            logger.warning(f"âš ï¸ ì˜ëª»ëœ íŒŒì¼ íƒ€ì…: {file.content_type}")
            return JSONResponse(
                status_code=400,
                content={
                    "status": "error",
                    "message": "ì˜¤ë””ì˜¤ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤"
                }
            )
        
        # íŒŒì¼ í¬ê¸° ê²€ì¦ (10MB ì œí•œ)
        max_size = 10 * 1024 * 1024  # 10MB
        file_content = await file.read()
        logger.info(f"ğŸ’¾ íŒŒì¼ í¬ê¸°: {len(file_content)} bytes")
        
        if len(file_content) > max_size:
            logger.warning(f"âš ï¸ íŒŒì¼ í¬ê¸° ì´ˆê³¼: {len(file_content)} > {max_size}")
            return JSONResponse(
                status_code=400,
                content={
                    "status": "error",
                    "message": "íŒŒì¼ í¬ê¸°ëŠ” 10MB ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
                }
            )
        
        # ëª©ì†Œë¦¬ ì¶”ê°€
        voice_id = await voice_manager.add_voice_from_file(file_content, file.filename)
        logger.info(f"âœ… ëª©ì†Œë¦¬ ì—…ë¡œë“œ ì„±ê³µ: {voice_id}")
        
        response_data = {
            "status": "success",
            "voice_id": voice_id,
            "message": "ëª©ì†Œë¦¬ ì—…ë¡œë“œ ì„±ê³µ",
            "filename": file.filename,
            "file_size_mb": round(len(file_content) / (1024 * 1024), 2),
            "timestamp": time.time()
        }
        
        return JSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"âŒ ëª©ì†Œë¦¬ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}",
                "error_type": type(e).__name__
            }
        )

@app.get("/api/tts/emotions")
async def get_emotion_presets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ê°ì • í”„ë¦¬ì…‹ ëª©ë¡ ë°˜í™˜"""
    try:
        presets = EmotionManager.get_available_presets()
        return {
            "status": "success",
            "emotion_presets": presets,
            "emotion_labels": [
                "Happiness", "Sadness", "Disgust", "Fear", 
                "Surprise", "Anger", "Other", "Neutral"
            ],
            "usage": {
                "emotion_preset": "ë„¤ì„ë“œ í”„ë¦¬ì…‹ ì‚¬ìš© (happy, sad, angry, ë“±)",
                "emotion": "ì»¤ìŠ¤í…€ ê°ì • ë²¡í„° [8ê°œ ê°’] ì‚¬ìš©",
                "enable_emotion": "ê°ì • íš¨ê³¼ í™œì„±í™”/ë¹„í™œì„±í™”"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/tts/voice-cache/clear")
async def clear_voice_cache():
    """ëª©ì†Œë¦¬ ìºì‹œ ì‚­ì œ"""
    try:
        voice_manager.clear_cache()
        return {
            "status": "success",
            "message": "ëª©ì†Œë¦¬ ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/tts/voice-cache/stats")
async def get_voice_cache_stats():
    """ëª©ì†Œë¦¬ ìºì‹œ í†µê³„"""
    try:
        stats = voice_manager.get_cache_stats()
        return {
            "status": "success",
            "cache_stats": stats
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ê°œì„ ëœ WebSocket TTS ì—”ë“œí¬ì¸íŠ¸
@app.websocket("/ws/tts/{client_id}")
async def enhanced_websocket_tts(websocket: WebSocket, client_id: str):
    """ê°œì„ ëœ WebSocket TTS ì—”ë“œí¬ì¸íŠ¸ - GPU/CPU ìµœì í™”"""
    await tts_manager.connect(websocket, client_id)
    
    # ì—°ê²° ì‹œ ìºì‹œ í†µê³„ ì „ì†¡
    cache_stats = tts_cache.get_cache_stats()
    
    # ì—°ê²° ì„±ê³µ ë©”ì‹œì§€ ì „ì†¡
    try:
        await websocket.send_json({
            "type": "connection_established",
            "client_id": client_id,
            "server_info": {
                "supported_models": getattr(app.state, 'supported_models', []),
                "device": str(device),
                "memory_info": model_cache._get_memory_usage(),
                "version": "2.1.0-optimized",
                "performance_mode": "ğŸš€ GPU ìµœì í™”" if device.type == "cuda" else "ğŸ”§ CPU",
                "cache_stats": cache_stats,
                "optimizations": {
                    "caching": True,
                    "parallel_processing": True,
                    "model_warmup": True,
                    "gpu_optimization": device.type == "cuda"
                }
            }
        })
        logger.info(f"âœ… í´ë¼ì´ì–¸íŠ¸ {client_id} ì—°ê²° ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ ì—°ê²° ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")
        tts_manager.disconnect(client_id)
        return
    
    try:
        while True:
            data = await websocket.receive_text()
            
            if data == "stop":
                try:
                    await websocket.send_json({
                        "type": "generation_stopped",
                        "message": "Audio generation stopped by user"
                    })
                except:
                    logger.warning(f"âš ï¸ stop ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨")
                continue
                
            if data == "ping":
                try:
                    await websocket.send_json({
                        "type": "pong",
                        "timestamp": time.time(),
                        "device": str(device)
                    })
                except:
                    logger.warning(f"âš ï¸ pong ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨")
                continue
                
            try:
                request_data = json.loads(data)
                text = request_data.get("text", "")
                model_choice = request_data.get("model", "Zyphra/Zonos-v0.1-transformer")
                format_type = request_data.get("format", "pcm")
                
                if not text.strip():
                    try:
                        await websocket.send_json({
                            "type": "error",
                            "error": "Empty text provided",
                            "error_code": "EMPTY_TEXT"
                        })
                    except:
                        logger.error(f"âŒ ë¹ˆ í…ìŠ¤íŠ¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨")
                    continue
                
                # ëª¨ë¸ ë¡œë“œ (í”„ë¡œê·¸ë ˆìŠ¤ë°” í¬í•¨)
                model = await model_cache.load_model_with_progress(model_choice, websocket)
                
                # ì‹œë“œ ì„¤ì •
                seed = request_data.get("seed", 420)
                if request_data.get("randomize_seed", True):
                    seed = torch.randint(0, 2**32 - 1, (1,)).item()
                torch.manual_seed(seed)
                
                # ğŸ¤ ëª©ì†Œë¦¬ ì²˜ë¦¬
                speaker_embedding = await voice_manager.process_voice_request(request_data, model)
                
                # ğŸ˜Š ê°ì • ì²˜ë¦¬ ê°œì„ 
                emotion_preset = request_data.get("emotion_preset")
                emotion_custom = request_data.get("emotion")
                emotion = EmotionManager.get_emotion_vector(emotion_preset, emotion_custom)
                enable_emotion = request_data.get("enable_emotion", True)
                
                # unconditional_keys ë™ì  ì„¤ì •
                unconditional_keys = {"vqscore_8", "dnsmos_ovrl"}
                if not enable_emotion:
                    unconditional_keys.add("emotion")
                
                # ì»¨ë””ì…”ë‹ ì„¤ì •
                cond_dict = make_cond_dict(
                    text=text,
                    language=request_data.get("language", "ko"),
                    speaker=speaker_embedding,  # ğŸ¤ ëª©ì†Œë¦¬ ì ìš©
                    emotion=emotion,  # ğŸ˜Š ê°ì • ì ìš©
                    fmax=request_data.get("fmax", 22050.0),
                    pitch_std=request_data.get("pitch_std", 20.0),
                    speaking_rate=request_data.get("speaking_rate", 15.0),
                    vqscore_8=request_data.get("vqscore_8", [0.78] * 8),
                    dnsmos_ovrl=request_data.get("dnsmos_ovrl", 4.0),
                    device=device,
                    unconditional_keys=unconditional_keys  # ë™ì  ì„¤ì •
                )
                
                conditioning = model.prepare_conditioning(cond_dict)
                
                # ğŸš€ ìš¸íŠ¸ë¼ ìµœì í™”ëœ ì˜¤ë””ì˜¤ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
                await ultra_optimized_stream_audio_generation(
                    websocket, model, conditioning, request_data, format_type, client_id
                )
                
                # ì™„ë£Œ ì‹ í˜¸
                try:
                    await websocket.send_json({
                        "type": "generation_complete",
                        "message": "Audio generation completed successfully",
                        "device": str(device)
                    })
                except Exception as e:
                    logger.warning(f"âš ï¸ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ ì‹¤íŒ¨: {e}")
                
            except json.JSONDecodeError:
                try:
                    await websocket.send_json({
                        "type": "error",
                        "error": "Invalid JSON format",
                        "error_code": "JSON_DECODE_ERROR"
                    })
                except:
                    logger.error(f"âŒ JSON ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨")
            except Exception as e:
                logger.error(f"âŒ TTS WebSocket error: {e}")
                try:
                    await websocket.send_json({
                        "type": "error",
                        "error": str(e),
                        "error_code": "TTS_ERROR"
                    })
                except:
                    logger.error(f"âŒ TTS ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨")
                
    except WebSocketDisconnect:
        tts_manager.disconnect(client_id)
    except Exception as e:
        logger.error(f"âŒ WebSocket connection error: {e}")
        tts_manager.disconnect(client_id)

# ìºì‹œ ë° ì„±ëŠ¥ API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/api/cache/stats")
async def get_cache_stats():
    """ìºì‹œ í†µê³„ ì¡°íšŒ"""
    return tts_cache.get_cache_stats()

@app.post("/api/cache/clear")
async def clear_cache():
    """ìºì‹œ ì™„ì „ ì‚­ì œ"""
    try:
        # ë©”ëª¨ë¦¬ ìºì‹œ í´ë¦¬ì–´
        tts_cache.memory_cache.clear()
        tts_cache.cache_metadata.clear()
        tts_cache.access_times.clear()
        
        # ë””ìŠ¤í¬ ìºì‹œ íŒŒì¼ë“¤ ì‚­ì œ
        import shutil
        if tts_cache.cache_dir.exists():
            shutil.rmtree(tts_cache.cache_dir)
            tts_cache.cache_dir.mkdir(parents=True, exist_ok=True)
        
        tts_cache.cache_hits = 0
        tts_cache.cache_misses = 0
        
        return {"status": "success", "message": "ìºì‹œê°€ ì™„ì „íˆ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}
    except Exception as e:
        return {"status": "error", "message": f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}"}

@app.get("/api/performance/stats")
async def get_performance_stats():
    """ì „ì²´ ì„±ëŠ¥ í†µê³„"""
    return {
        "cache_stats": tts_cache.get_cache_stats(),
        "gpu_info": {
            "available": torch.cuda.is_available(),
            "device_name": torch.cuda.get_device_name() if torch.cuda.is_available() else None,
            "memory_usage": model_cache._get_memory_usage()
        },
        "model_info": {
            "loaded_models": list(model_cache.models.keys()),
            "warmed_up_models": list(model_cache.warmup_completed) if hasattr(model_cache, 'warmup_completed') else []
        },
        "optimization_features": {
            "advanced_caching": True,
            "parallel_processing": True,
            "model_warmup": True,
            "gpu_optimization": device.type == "cuda",
            "mixed_precision": os.getenv("MIXED_PRECISION", "true").lower() == "true"
        }
    }

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„œë²„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    reload = os.getenv("RELOAD", "false").lower() == "true"
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=reload,
        log_level=log_level.lower()
    )