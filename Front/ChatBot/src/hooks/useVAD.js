import { useRef, useCallback, useState, useEffect } from 'react';

// VAD (Voice Activity Detection) ê¸°ëŠ¥ì´ ìˆëŠ” PCM ë ˆì½”ë”
export const useVADRecorder = () => {
  const audioContextRef = useRef(null);
  const processorRef = useRef(null);
  const streamRef = useRef(null);
  const analyserRef = useRef(null);
  
  const [isRecording, setIsRecording] = useState(false);
  const [error, setError] = useState(null);
  const [voiceLevel, setVoiceLevel] = useState(0);
  const [isVoiceDetected, setIsVoiceDetected] = useState(false);
  
  // VAD ì„¤ì •
  const vadConfigRef = useRef({
    voiceThreshold: 0.06,     // ğŸ”¥ ìŒì„± ê°ì§€ ì„ê³„ê°’ 0.1 â†’ 0.06ìœ¼ë¡œ ë” ë¯¼ê°í•˜ê²Œ
    silenceThreshold: 0.025,  // ğŸ”¥ ì •ì  ê°ì§€ ì„ê³„ê°’ 0.05 â†’ 0.025ë¡œ ë” ë¯¼ê°í•˜ê²Œ
    minVoiceDuration: 250,    // ğŸ”¥ ìµœì†Œ ìŒì„± ì§€ì† ì‹œê°„ 300 â†’ 250ìœ¼ë¡œ ë” ë¹ ë¥´ê²Œ
    maxSilenceDuration: 1000, // ğŸ”¥ ìµœëŒ€ ì •ì  ì§€ì† ì‹œê°„ 1500 â†’ 1000ìœ¼ë¡œ ë” ë¹ ë¥´ê²Œ
    bufferDuration: 400       // ğŸ”¥ ë…¹ìŒ ì‹œì‘ ì „ ë²„í¼ ì‹œê°„ 500 â†’ 400ìœ¼ë¡œ ë” ë¹ ë¥´ê²Œ
  });
  
  // VAD ìƒíƒœ ì¶”ì 
  const vadStateRef = useRef({
    isVoiceActive: false,
    voiceStartTime: 0,
    silenceStartTime: 0,
    lastVoiceTime: 0,
    audioBuffer: []
  });
  
  const onPCMDataRef = useRef(null);
  const onVoiceStartRef = useRef(null);
  const onVoiceEndRef = useRef(null);
  const onSilenceDetectedRef = useRef(null);
  
  // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ í•¨ìˆ˜
  const analyzeAudioLevel = useCallback((audioData) => {
    if (!analyserRef.current) return 0;
    
    const bufferLength = analyserRef.current.fftSize;
    const dataArray = new Float32Array(bufferLength);
    analyserRef.current.getFloatTimeDomainData(dataArray);
    
    // RMS (Root Mean Square) ê³„ì‚°
    let sum = 0;
    for (let i = 0; i < bufferLength; i++) {
      sum += dataArray[i] * dataArray[i];
    }
    const rms = Math.sqrt(sum / bufferLength);
    
    return rms;
  }, []);
  
  // VAD ë¡œì§ ì²˜ë¦¬
  const processVAD = useCallback((audioLevel) => {
    const now = Date.now();
    const config = vadConfigRef.current;
    const state = vadStateRef.current;
    
    setVoiceLevel(audioLevel);
    
    if (audioLevel > config.voiceThreshold) {
      // ìŒì„± ê°ì§€ë¨
      if (!state.isVoiceActive) {
        // ìŒì„± ì‹œì‘
        state.isVoiceActive = true;
        state.voiceStartTime = now;
        setIsVoiceDetected(true);
        
        if (onVoiceStartRef.current) {
          onVoiceStartRef.current();
        }
        
        console.log('ğŸ¤ ìŒì„± ê°ì§€ ì‹œì‘ - ë ˆë²¨:', audioLevel.toFixed(3));
      }
      
      state.lastVoiceTime = now;
      state.silenceStartTime = 0; // ì •ì  íƒ€ì´ë¨¸ ë¦¬ì…‹
      
    } else if (audioLevel < config.silenceThreshold) {
      // ì •ì  ê°ì§€ë¨
      if (state.isVoiceActive) {
        const voiceDuration = now - state.voiceStartTime;
        
        if (voiceDuration >= config.minVoiceDuration) {
          // ì¶©ë¶„í•œ ìŒì„±ì´ ê°ì§€ë˜ì—ˆìŒ
          if (state.silenceStartTime === 0) {
            state.silenceStartTime = now;
          }
          
          const silenceDuration = now - state.silenceStartTime;
          
          if (silenceDuration >= config.maxSilenceDuration) {
            // ì¶©ë¶„í•œ ì •ì ì´ ì§€ì†ë¨ - ìŒì„± ì¢…ë£Œ
            state.isVoiceActive = false;
            setIsVoiceDetected(false);
            
            if (onVoiceEndRef.current) {
              onVoiceEndRef.current();
            }
            
            if (onSilenceDetectedRef.current) {
              onSilenceDetectedRef.current();
            }
            
            console.log('ğŸ”‡ ìŒì„± ì¢…ë£Œ ê°ì§€ - ìŒì„± ê¸¸ì´:', voiceDuration, 'ms, ì •ì  ê¸¸ì´:', silenceDuration, 'ms');
          }
        }
      }
    }
    
    // ì¤‘ê°„ ë ˆë²¨ (ì„ê³„ê°’ ì‚¬ì´)
    if (audioLevel >= config.silenceThreshold && audioLevel <= config.voiceThreshold) {
      // ì„ê³„ê°’ ì‚¬ì´ì˜ ëª¨í˜¸í•œ ë ˆë²¨ì€ í˜„ì¬ ìƒíƒœ ìœ ì§€
      if (state.isVoiceActive && state.silenceStartTime > 0) {
        const silenceDuration = now - state.silenceStartTime;
        if (silenceDuration >= config.maxSilenceDuration) {
          const voiceDuration = now - state.voiceStartTime;
          if (voiceDuration >= config.minVoiceDuration) {
            state.isVoiceActive = false;
            setIsVoiceDetected(false);
            
            if (onVoiceEndRef.current) {
              onVoiceEndRef.current();
            }
            
            if (onSilenceDetectedRef.current) {
              onSilenceDetectedRef.current();
            }
            
            console.log('ğŸ”‡ ìŒì„± ì¢…ë£Œ ê°ì§€ (ì¤‘ê°„ ë ˆë²¨) - ìŒì„± ê¸¸ì´:', voiceDuration, 'ms');
          }
        }
      }
    }
  }, []);
  
  const startRecording = useCallback(async (onPCMData, options = {}) => {
    try {
      setError(null);
      
      // ì½œë°± ì €ì¥
      onPCMDataRef.current = onPCMData;
      onVoiceStartRef.current = options.onVoiceStart;
      onVoiceEndRef.current = options.onVoiceEnd;
      onSilenceDetectedRef.current = options.onSilenceDetected;
      
      // VAD ì„¤ì • ì—…ë°ì´íŠ¸
      if (options.vadConfig) {
        vadConfigRef.current = { ...vadConfigRef.current, ...options.vadConfig };
      }
      
      // VAD ìƒíƒœ ì´ˆê¸°í™”
      vadStateRef.current = {
        isVoiceActive: false,
        voiceStartTime: 0,
        silenceStartTime: 0,
        lastVoiceTime: 0,
        audioBuffer: []
      };
      
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 16000
        });
      }
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: false // VADë¥¼ ìœ„í•´ ìë™ ê²Œì¸ ì»¨íŠ¸ë¡¤ ë¹„í™œì„±í™”
        }
      });
      
      streamRef.current = stream;
      
      const source = audioContextRef.current.createMediaStreamSource(stream);
      
      // ë¶„ì„ê¸° ìƒì„± (VADìš©)
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 2048;
      analyserRef.current.smoothingTimeConstant = 0.8;
      
      source.connect(analyserRef.current);
      
      // í”„ë¡œì„¸ì„œ ìƒì„± (PCM ë°ì´í„° ì¶”ì¶œìš©)
      processorRef.current = audioContextRef.current.createScriptProcessor(4096, 1, 1);
      
      processorRef.current.onaudioprocess = (event) => {
        const inputBuffer = event.inputBuffer;
        const inputData = inputBuffer.getChannelData(0);
        
        // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„
        const audioLevel = analyzeAudioLevel(inputData);
        processVAD(audioLevel);
        
        // PCM ë°ì´í„° ë³€í™˜
        const int16Array = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
          int16Array[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
        }
        
        // PCM ë°ì´í„° ì „ì†¡
        if (onPCMDataRef.current) {
          onPCMDataRef.current(int16Array.buffer);
        }
      };
      
      analyserRef.current.connect(processorRef.current);
      processorRef.current.connect(audioContextRef.current.destination);
      
      setIsRecording(true);
      setIsVoiceDetected(false);
      
      console.log('ğŸ¤ VAD ë ˆì½”ë” ì‹œì‘ë¨');
      
    } catch (err) {
      console.error('âŒ VAD ë…¹ìŒ ì‹œì‘ ì˜¤ë¥˜:', err);
      setError(err.message);
      setIsRecording(false);
    }
  }, [analyzeAudioLevel, processVAD]);
  
  const stopRecording = useCallback(() => {
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }
    
    if (analyserRef.current) {
      analyserRef.current.disconnect();
      analyserRef.current = null;
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    // VAD ìƒíƒœ ì´ˆê¸°í™”
    vadStateRef.current = {
      isVoiceActive: false,
      voiceStartTime: 0,
      silenceStartTime: 0,
      lastVoiceTime: 0,
      audioBuffer: []
    };
    
    setIsRecording(false);
    setIsVoiceDetected(false);
    setVoiceLevel(0);
    
    console.log('ğŸ”‡ VAD ë ˆì½”ë” ì¤‘ì§€ë¨');
  }, []);
  
  const updateVADConfig = useCallback((newConfig) => {
    vadConfigRef.current = { ...vadConfigRef.current, ...newConfig };
    console.log('âš™ï¸ VAD ì„¤ì • ì—…ë°ì´íŠ¸ë¨:', vadConfigRef.current);
  }, []);
  
  useEffect(() => {
    return () => {
      stopRecording();
    };
  }, [stopRecording]);
  
  return {
    startRecording,
    stopRecording,
    updateVADConfig,
    isRecording,
    isVoiceDetected,
    voiceLevel,
    vadConfig: vadConfigRef.current,
    error
  };
};

// ê¸°ì¡´ usePCMRecorderë„ ë‚´ë³´ë‚´ê¸° (í•˜ìœ„ í˜¸í™˜ì„±)
export const usePCMRecorder = () => {
  const audioContextRef = useRef(null);
  const processorRef = useRef(null);
  const streamRef = useRef(null);
  const [isRecording, setIsRecording] = useState(false);
  const [error, setError] = useState(null);
  
  const startRecording = useCallback(async (onPCMData) => {
    try {
      setError(null);
      
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 16000
        });
      }
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      });
      
      streamRef.current = stream;
      
      const source = audioContextRef.current.createMediaStreamSource(stream);
      
      processorRef.current = audioContextRef.current.createScriptProcessor(4096, 1, 1);
      
      processorRef.current.onaudioprocess = (event) => {
        const inputBuffer = event.inputBuffer;
        const inputData = inputBuffer.getChannelData(0);
        
        const int16Array = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
          int16Array[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
        }
        
        onPCMData?.(int16Array.buffer);
      };
      
      source.connect(processorRef.current);
      processorRef.current.connect(audioContextRef.current.destination);
      
      setIsRecording(true);
      
    } catch (err) {
      console.error('PCM ë…¹ìŒ ì‹œì‘ ì˜¤ë¥˜:', err);
      setError(err.message);
      setIsRecording(false);
    }
  }, []);
  
  const stopRecording = useCallback(() => {
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    setIsRecording(false);
  }, []);
  
  useEffect(() => {
    return () => {
      stopRecording();
    };
  }, [stopRecording]);
  
  return {
    startRecording,
    stopRecording,
    isRecording,
    error
  };
};
