import { useRef, useCallback, useEffect, useState } from 'react';

export const useAudioPlayer = () => {
  const audioContextRef = useRef(null);
  const gainNodeRef = useRef(null);
  const currentSourceRef = useRef(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const [volume, setVolume] = useState(1.0);
  const [debugInfo, setDebugInfo] = useState('');
  
  useEffect(() => {
    // AudioContext ì´ˆê¸°í™” - ê³ ì •ëœ ìƒ˜í”Œë ˆì´íŠ¸ ì‚¬ìš©
    if (!audioContextRef.current) {
      try {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 24000, // ğŸ”¥ ì„œë²„ì™€ ë™ì¼í•œ ìƒ˜í”Œë ˆì´íŠ¸ë¡œ ê³ ì •
          latencyHint: 'interactive'
        });
        gainNodeRef.current = audioContextRef.current.createGain();
        gainNodeRef.current.connect(audioContextRef.current.destination);
        gainNodeRef.current.gain.value = volume;
        
        console.log('ğŸµ AudioContext ì´ˆê¸°í™” ì™„ë£Œ:', {
          sampleRate: audioContextRef.current.sampleRate,
          state: audioContextRef.current.state
        });
        setDebugInfo(`AudioContext ìƒì„±ë¨ (${audioContextRef.current.sampleRate}Hz)`);
      } catch (error) {
        console.error('âŒ AudioContext ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
        setDebugInfo(`AudioContext ì´ˆê¸°í™” ì‹¤íŒ¨: ${error.message}`);
      }
    }
    
    return () => {
      stop();
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        audioContextRef.current.close();
      }
    };
  }, []);
  
  useEffect(() => {
    if (gainNodeRef.current) {
      gainNodeRef.current.gain.value = volume;
    }
  }, [volume]);
  
  // ğŸ”¥ í˜„ì¬ ì˜¤ë””ì˜¤ ì¤‘ë‹¨ í•¨ìˆ˜ (ê°œì„ ë¨)
  const stopCurrentAudio = useCallback(() => {
    if (currentSourceRef.current) {
      try {
        currentSourceRef.current.stop();
        console.log('ğŸ”‡ í˜„ì¬ ì˜¤ë””ì˜¤ ì†ŒìŠ¤ ì¤‘ë‹¨ë¨');
      } catch (e) {
        // ì´ë¯¸ ì¤‘ë‹¨ëœ ê²½ìš° ë¬´ì‹œ
        console.log('â„¹ï¸ ì˜¤ë””ì˜¤ ì†ŒìŠ¤ ì´ë¯¸ ì¤‘ë‹¨ë¨');
      }
      currentSourceRef.current = null;
    }
    setIsPlaying(false);
  }, []);
  
  // ğŸ”¥ ì™„ì „íˆ ìƒˆë¡­ê²Œ ì‘ì„±ëœ PCM ì¬ìƒ í•¨ìˆ˜ - ë””ë²„ê¹… ê°•í™”
  const playPCMChunk = useCallback(async (arrayBuffer, inputSampleRate = 24000) => {
    console.log('ğŸµ PCM ì¬ìƒ ì‹œì‘:', {
      bufferSize: arrayBuffer.byteLength,
      inputSampleRate: inputSampleRate,
      audioContextState: audioContextRef.current?.state,
      audioContextSampleRate: audioContextRef.current?.sampleRate
    });
    
    if (!audioContextRef.current || !gainNodeRef.current) {
      console.error('âŒ AudioContextê°€ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ');
      setDebugInfo('AudioContextê°€ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ');
      return;
    }
    
    // AudioContext ìƒíƒœ í™•ì¸ ë° ì¬ê°œ
    if (audioContextRef.current.state === 'suspended') {
      try {
        await audioContextRef.current.resume();
        console.log('ğŸ”Š AudioContext ì¬ê°œë¨');
        setDebugInfo('AudioContext ì¬ê°œë¨');
      } catch (error) {
        console.error('âŒ AudioContext ì¬ê°œ ì‹¤íŒ¨:', error);
        setDebugInfo(`AudioContext ì¬ê°œ ì‹¤íŒ¨: ${error.message}`);
        return;
      }
    }
    
    try {
      // ğŸ”¥ ì´ì „ ì˜¤ë””ì˜¤ ì¦‰ì‹œ ì¤‘ë‹¨
      stopCurrentAudio();
      
      // ğŸ”¥ ë””ë²„ê¹…: ArrayBuffer ë‚´ìš© ê²€ì‚¬
      const uint8View = new Uint8Array(arrayBuffer.slice(0, 20));
      console.log('ğŸ” ArrayBuffer ì²˜ìŒ 20ë°”ì´íŠ¸:', Array.from(uint8View).map(b => b.toString(16).padStart(2, '0')).join(' '));
      
      // Int16Arrayë¡œ ë³€í™˜
      const int16Array = new Int16Array(arrayBuffer);
      console.log('ğŸ“Š ì˜¤ë””ì˜¤ ë°ì´í„° ë¶„ì„:', {
        samples: int16Array.length,
        duration: int16Array.length / inputSampleRate,
        maxValue: Math.max(...int16Array.slice(0, Math.min(1000, int16Array.length))),
        minValue: Math.min(...int16Array.slice(0, Math.min(1000, int16Array.length))),
        firstFewSamples: Array.from(int16Array.slice(0, 10))
      });
      
      // ğŸ”¥ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì¡°ìš©í•œì§€ í™•ì¸
      const maxSample = Math.max(...int16Array.slice(0, Math.min(1000, int16Array.length)));
      const minSample = Math.min(...int16Array.slice(0, Math.min(1000, int16Array.length)));
      if (Math.abs(maxSample) < 100 && Math.abs(minSample) < 100) {
        console.warn('âš ï¸ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ë§¤ìš° ì¡°ìš©í•©ë‹ˆë‹¤ (max:', maxSample, ', min:', minSample, ')');
        setDebugInfo('ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ë§¤ìš° ì¡°ìš©í•¨');
      }
      
      // Float32Arrayë¡œ ë³€í™˜
      const float32Array = new Float32Array(int16Array.length);
      for (let i = 0; i < int16Array.length; i++) {
        float32Array[i] = int16Array[i] / 32768.0;
      }
      
      // ğŸ”¥ AudioBuffer ìƒì„± - ìƒ˜í”Œë ˆì´íŠ¸ ë§ì¶¤
      const audioContextSampleRate = audioContextRef.current.sampleRate;
      let finalAudioBuffer;
      
      if (inputSampleRate === audioContextSampleRate) {
        // ìƒ˜í”Œë ˆì´íŠ¸ê°€ ê°™ìœ¼ë©´ ë°”ë¡œ ì‚¬ìš©
        finalAudioBuffer = audioContextRef.current.createBuffer(
          1, // ëª¨ë…¸
          float32Array.length,
          audioContextSampleRate
        );
        finalAudioBuffer.getChannelData(0).set(float32Array);
        console.log('âœ… ìƒ˜í”Œë ˆì´íŠ¸ ì¼ì¹˜ - ë°”ë¡œ ì‚¬ìš©');
      } else {
        // ìƒ˜í”Œë ˆì´íŠ¸ê°€ ë‹¤ë¥´ë©´ ë¦¬ìƒ˜í”Œë§
        console.log('ğŸ”„ ìƒ˜í”Œë ˆì´íŠ¸ ë³€í™˜ ì‹œì‘:', inputSampleRate, 'â†’', audioContextSampleRate);
        
        const ratio = audioContextSampleRate / inputSampleRate;
        const newLength = Math.floor(float32Array.length * ratio);
        const resampledArray = new Float32Array(newLength);
        
        // ì„ í˜• ë³´ê°„ ë¦¬ìƒ˜í”Œë§
        for (let i = 0; i < newLength; i++) {
          const sourceIndex = i / ratio;
          const index = Math.floor(sourceIndex);
          const fraction = sourceIndex - index;
          
          if (index + 1 < float32Array.length) {
            resampledArray[i] = float32Array[index] * (1 - fraction) + float32Array[index + 1] * fraction;
          } else {
            resampledArray[i] = float32Array[index] || 0;
          }
        }
        
        finalAudioBuffer = audioContextRef.current.createBuffer(
          1,
          newLength,
          audioContextSampleRate
        );
        finalAudioBuffer.getChannelData(0).set(resampledArray);
        
        console.log('âœ… ë¦¬ìƒ˜í”Œë§ ì™„ë£Œ:', {
          originalLength: float32Array.length,
          newLength: newLength,
          ratio: ratio
        });
      }
      
      // ğŸ”¥ AudioBuffer ë‚´ìš© ê²€ì‚¬
      const bufferData = finalAudioBuffer.getChannelData(0);
      const bufferMax = Math.max(...bufferData.slice(0, Math.min(1000, bufferData.length)));
      const bufferMin = Math.min(...bufferData.slice(0, Math.min(1000, bufferData.length)));
      console.log('ğŸ¶ ìµœì¢… AudioBuffer ë¶„ì„:', {
        duration: finalAudioBuffer.duration,
        sampleRate: finalAudioBuffer.sampleRate,
        length: finalAudioBuffer.length,
        maxValue: bufferMax,
        minValue: bufferMin,
        firstFewSamples: Array.from(bufferData.slice(0, 10))
      });
      
      if (Math.abs(bufferMax) < 0.01 && Math.abs(bufferMin) < 0.01) {
        console.warn('âš ï¸ ìµœì¢… AudioBufferê°€ ë§¤ìš° ì¡°ìš©í•©ë‹ˆë‹¤!');
        setDebugInfo('ìµœì¢… AudioBufferê°€ ë§¤ìš° ì¡°ìš©í•¨');
      }
      
      // ğŸ”¥ ì˜¤ë””ì˜¤ ì†ŒìŠ¤ ìƒì„± ë° ì¬ìƒ
      const source = audioContextRef.current.createBufferSource();
      source.buffer = finalAudioBuffer;
      source.connect(gainNodeRef.current);
      
      // í˜„ì¬ ì†ŒìŠ¤ë¡œ ì„¤ì •
      currentSourceRef.current = source;
      setIsPlaying(true);
      
      // ì¬ìƒ ì™„ë£Œ ì´ë²¤íŠ¸
      source.onended = () => {
        console.log('âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ');
        if (currentSourceRef.current === source) {
          currentSourceRef.current = null;
          setIsPlaying(false);
        }
      };
      
      // ì˜¤ë¥˜ ì´ë²¤íŠ¸
      source.onerror = (error) => {
        console.error('âŒ ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜:', error);
        if (currentSourceRef.current === source) {
          currentSourceRef.current = null;
          setIsPlaying(false);
        }
        setDebugInfo(`ì¬ìƒ ì˜¤ë¥˜: ${error.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
      };
      
      // ì¬ìƒ ì‹œì‘
      source.start();
      
      console.log('ğŸ¶ ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘:', {
        duration: finalAudioBuffer.duration,
        sampleRate: finalAudioBuffer.sampleRate,
        gainValue: gainNodeRef.current.gain.value
      });
      
      setDebugInfo(`ì¬ìƒ ì¤‘: ${finalAudioBuffer.duration.toFixed(2)}ì´ˆ, ${finalAudioBuffer.sampleRate}Hz, ë³¼ë¥¨: ${gainNodeRef.current.gain.value}`);
      
    } catch (error) {
      console.error('âŒ PCM ì¬ìƒ ì˜¤ë¥˜:', error);
      console.error('ì˜¤ë¥˜ ìŠ¤íƒ:', error.stack);
      setDebugInfo(`ì¬ìƒ ì˜¤ë¥˜: ${error.message}`);
      setIsPlaying(false);
    }
  }, [stopCurrentAudio]);
  
  // ğŸ”¥ ì „ì²´ ì¤‘ë‹¨ í•¨ìˆ˜
  const stop = useCallback(() => {
    stopCurrentAudio();
    console.log('ğŸ”‡ ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ ì¤‘ë‹¨');
  }, [stopCurrentAudio]);
  
  // ğŸ”¥ ìƒˆ ì˜¤ë””ì˜¤ ì‹œì‘ ì‹œ ì´ì „ ì˜¤ë””ì˜¤ ì¤‘ë‹¨
  const startNewAudio = useCallback(() => {
    stopCurrentAudio();
    console.log('ğŸµ ìƒˆ ì˜¤ë””ì˜¤ ì‹œì‘ - ì´ì „ ì˜¤ë””ì˜¤ ì¤‘ë‹¨');
  }, [stopCurrentAudio]);
  
  // ğŸ”¥ í…ŒìŠ¤íŠ¸ìš© ì˜¤ë””ì˜¤ ì¬ìƒ í•¨ìˆ˜ (ê°œì„ ë¨)
  const testAudioPlayback = useCallback(async () => {
    console.log('ğŸ§ ì˜¤ë””ì˜¤ ì¬ìƒ í…ŒìŠ¤íŠ¸ ì‹œì‘');
    
    try {
      if (!audioContextRef.current) {
        setDebugInfo('ì˜¤ë¥˜: AudioContextê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ');
        return;
      }
      
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
        console.log('ğŸ”Š AudioContext ì¬ê°œë¨');
      }
      
      // í…ŒìŠ¤íŠ¸ìš© 440Hz ì‚¬ì¸íŒŒ ìƒì„± (1ì´ˆ)
      const sampleRate = audioContextRef.current.sampleRate;
      const duration = 1;
      const frequency = 440;
      
      const buffer = audioContextRef.current.createBuffer(1, sampleRate * duration, sampleRate);
      const channelData = buffer.getChannelData(0);
      
      for (let i = 0; i < channelData.length; i++) {
        channelData[i] = Math.sin(2 * Math.PI * frequency * i / sampleRate) * 0.3;
      }
      
      // ì´ì „ ì˜¤ë””ì˜¤ ì¤‘ë‹¨
      stopCurrentAudio();
      
      const source = audioContextRef.current.createBufferSource();
      source.buffer = buffer;
      source.connect(gainNodeRef.current);
      
      currentSourceRef.current = source;
      setIsPlaying(true);
      
      source.onended = () => {
        console.log('âœ… í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ');
        setDebugInfo('í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ');
        setIsPlaying(false);
        currentSourceRef.current = null;
      };
      
      source.start();
      
      console.log('ğŸµ í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘');
      setDebugInfo('í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘... (440Hz ì‚¬ì¸íŒŒ)');
      
    } catch (error) {
      console.error('âŒ í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨:', error);
      setDebugInfo(`í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ì‹¤íŒ¨: ${error.message}`);
    }
  }, [stopCurrentAudio]);
  
  const resumeContext = useCallback(async () => {
    if (audioContextRef.current && audioContextRef.current.state === 'suspended') {
      await audioContextRef.current.resume();
      console.log('ğŸ”Š AudioContext ì¬ê°œë¨');
    }
  }, []);
  
  return {
    playPCMChunk,
    stop,
    startNewAudio,
    resumeContext,
    testAudioPlayback,
    isPlaying,
    volume,
    setVolume,
    debugInfo,
    audioContextState: audioContextRef.current?.state || 'not_created'
  };
};

export const useAudioRecorder = () => {
  const mediaRecorderRef = useRef(null);
  const streamRef = useRef(null);
  const [isRecording, setIsRecording] = useState(false);
  const [error, setError] = useState(null);
  
  const startRecording = useCallback(async (onDataAvailable) => {
    try {
      setError(null);
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      });
      
      streamRef.current = stream;
      
      mediaRecorderRef.current = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          onDataAvailable?.(event.data);
        }
      };
      
      mediaRecorderRef.current.onstart = () => {
        setIsRecording(true);
      };
      
      mediaRecorderRef.current.onstop = () => {
        setIsRecording(false);
      };
      
      mediaRecorderRef.current.start(100);
      
    } catch (err) {
      console.error('ë…¹ìŒ ì‹œì‘ ì˜¤ë¥˜:', err);
      setError(err.message);
      setIsRecording(false);
    }
  }, []);
  
  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    setIsRecording(false);
  }, [isRecording]);
  
  useEffect(() => {
    return () => {
      stopRecording();
    };
  }, [stopRecording]);
  
  return {
    startRecording,
    stopRecording,
    isRecording,
    error
  };
};

export const usePCMRecorder = () => {
  const audioContextRef = useRef(null);
  const processorRef = useRef(null);
  const streamRef = useRef(null);
  const [isRecording, setIsRecording] = useState(false);
  const [error, setError] = useState(null);
  
  const startRecording = useCallback(async (onPCMData) => {
    try {
      setError(null);
      
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 16000
        });
      }
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      });
      
      streamRef.current = stream;
      
      const source = audioContextRef.current.createMediaStreamSource(stream);
      
      processorRef.current = audioContextRef.current.createScriptProcessor(4096, 1, 1);
      
      processorRef.current.onaudioprocess = (event) => {
        const inputBuffer = event.inputBuffer;
        const inputData = inputBuffer.getChannelData(0);
        
        const int16Array = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
          int16Array[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
        }
        
        onPCMData?.(int16Array.buffer);
      };
      
      source.connect(processorRef.current);
      processorRef.current.connect(audioContextRef.current.destination);
      
      setIsRecording(true);
      
    } catch (err) {
      console.error('PCM ë…¹ìŒ ì‹œì‘ ì˜¤ë¥˜:', err);
      setError(err.message);
      setIsRecording(false);
    }
  }, []);
  
  const stopRecording = useCallback(() => {
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    setIsRecording(false);
  }, []);
  
  useEffect(() => {
    return () => {
      stopRecording();
    };
  }, [stopRecording]);
  
  return {
    startRecording,
    stopRecording,
    isRecording,
    error
  };
};